# mollyhkim July-August 2017

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from oauth2client.service_account import ServiceAccountCredentials
from six.moves import urllib

import argparse
import sys
import tempfile
import gspread
import json
import csv
import shutil

import pandas as pd
import tensorflow as tf
import numpy as np

flags = tf.app.flags
FLAGS = flags.FLAGS

flags.DEFINE_string("model_dir", "", "Base directory for output models.")
flags.DEFINE_string("model_type", "wide_n_deep",
                    "Valid model types: {'wide', 'deep', 'wide_n_deep'}.")
flags.DEFINE_integer("train_steps", 200, "Number of training steps.")
flags.DEFINE_string(
    "train_data",
    "",
    "Path to the training data.")
flags.DEFINE_string(
    "test_data",
    "",
    "Path to the test data.")

COLUMNS = ["wealth_tier", "num_curr_accounts", "record_type", "net_prom_score", "wallet_share", 
           "transaction_trends", "deepen_reltn"]
LABEL_COLUMN = "label"
CATEGORICAL_COLUMNS = ["wealth_tier", "record_type"]
CONTINUOUS_COLUMNS = ["net_prom_score", "wallet_share", "num_curr_accounts", 
                      "transaction_trends"]

def maybe_download():
  if FLAGS.train_data:
    train_file_name = FLAGS.train_data
  else:
    train_file = tempfile.NamedTemporaryFile(delete=False)
    train_file_name = 'dummydata_train.csv'
    #train_file_name = train_file.name
    train_file.close()
    #print("Training data is downloaded to %s" % train_file_name)

  if FLAGS.test_data:
    test_file_name = FLAGS.test_data
  else:
    test_file = tempfile.NamedTemporaryFile(delete=False)
    test_file_name = 'dummydata_test.csv'
    #test_file_name = test_file.name
    test_file.close()
    #print("Test data is downloaded to %s" % test_file_name)

    
    '''
    data=[]
    with open('dummydata_train.csv') as f:
      reader=csv.reader(f)
      for r in reader:
        data.append(r)

    SCOPE = ["https://spreadsheets.google.com/feeds"]
    SECRETS_FILE = "Salesforce Data-6c84ebef2ffc.json"
    SPREADSHEET = "Salesforce data"

    json_key = json.load(open(SECRETS_FILE))
    credentials = ServiceAccountCredentials.from_json_keyfile_name(SECRETS_FILE, SCOPE)
    gc = gspread.authorize(credentials)
    sheet = gc.open(SPREADSHEET).sheet1
    COLUMNS = []#sheet.row_values(1)
    all_cells=sheet.range('A1:H1')
    for cell in all_cells:
      COLUMNS.append(cell.value)
      
  print('printingcols')
  print(CATEGORICAL_COLUMNS)
  print(CONTINUOUS_COLUMNS)
  '''

  return train_file_name, test_file_name

def build_estimator(model_dir):
  """Build an estimator."""
 # wealth_tier = tf.contrib.layers.sparse_column_with_keys(column_name="wealth_tier",
 #                                                         keys=["A+", "A", "B", "C", "D"])
  wealth_tier = tf.contrib.layers.sparse_column_with_hash_bucket(
      "wealth_tier", hash_bucket_size=1000)
  record_type = tf.contrib.layers.sparse_column_with_hash_bucket(
      "record_type", hash_bucket_size=1000)

  net_prom_score = tf.contrib.layers.real_valued_column("net_prom_score")
  wallet_share = tf.contrib.layers.real_valued_column("wallet_share")
  num_curr_accounts = tf.contrib.layers.real_valued_column("num_curr_accounts")
  transaction_trends = tf.contrib.layers.real_valued_column("transaction_trends")
  
  wide_columns = [wealth_tier, record_type]#, net_prom_score, wallet_share, num_curr_accounts, transaction_trends]
  deep_columns = [
      tf.contrib.layers.embedding_column(wealth_tier, dimension=8), 
      tf.contrib.layers.embedding_column(record_type, dimension=8), 
      net_prom_score, 
      wallet_share, 
      num_curr_accounts, 
      transaction_trends
  ]
  
  if FLAGS.model_type == "wide":
    m = tf.contrib.learn.LinearClassifier(model_dir=model_dir,
                                          feature_columns=wide_columns)
  elif FLAGS.model_type == "deep":
    m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,
                                       feature_columns=deep_columns,
                                       hidden_units=[100, 50])
  else:
    m = tf.contrib.learn.DNNLinearCombinedClassifier(
        model_dir=model_dir,
        linear_feature_columns=wide_columns,
        dnn_feature_columns=deep_columns,
        dnn_hidden_units=[100, 50])
        #n_classes=17)
  return m


def input_fn(df):
  """Input builder function."""
  # Creates a dictionary mapping from each continuous feature column name (k) to
  # the values of that column stored in a constant Tensor.
  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}
  # Creates a dictionary mapping from each categorical feature column name (k)
  # to the values of that column stored in a tf.SparseTensor.

  for k in CATEGORICAL_COLUMNS:
    print('print vals')
    print(df[k].values) 
  categorical_cols = {k: tf.SparseTensor(
      indices=[[i, 0] for i in range(df[k].size)],
      values=df[k].values,
      #changed from dense_shape (see https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!msg/discuss/OePXmC9kJ7o/SRErOoYCDQAJ)
      shape=[df[k].size, 1])
                      for k in CATEGORICAL_COLUMNS}

  # Merges the two dictionaries into one.
  feature_cols = dict(continuous_cols)
  feature_cols.update(categorical_cols)
  label= tf.constant(df[LABEL_COLUMN].values)
  # Converts the label column into a constant Tensor.
  #label = tf.constant(df[LABEL_COLUMN].values)
  '''  
  intval=[]
  for i in df[LABEL_COLUMN].values:
    if (i=='True'):
      intval.append(1)
    else:
      intval.append(0)
  print('printing intvals')
  print(intval)
  
  label = tf.constant(intval)
  '''
  # Returns the feature columns and the label.
  return feature_cols, label

def train_and_eval():
  """Train and evaluate the model."""
  train_file_name, test_file_name = maybe_download()
  df_train = pd.read_csv(
      tf.gfile.Open(train_file_name),
      names=COLUMNS,
      skipinitialspace=True,
      engine="python")
  df_test = pd.read_csv(
      tf.gfile.Open(test_file_name),
      names=COLUMNS,
      skipinitialspace=True,
      skiprows=1,
      engine="python")
  print('PRINTING Dtypes')
  print(df_test.dtypes)

  # remove NaN elements
  df_train = df_train.dropna(how='any', axis=0)
  df_test = df_test.dropna(how='any', axis=0)

  df_train[LABEL_COLUMN] = (
      df_train["deepen_reltn"].apply(lambda x: "Yes" in x))
  df_test[LABEL_COLUMN] = (
      df_test["deepen_reltn"].apply(lambda x: "Yes" in x))

  model_dir = tempfile.mkdtemp() if not FLAGS.model_dir else FLAGS.model_dir
  print("model directory = %s" % model_dir)
  
  m = build_estimator(model_dir)
  print('PRINTING DFTRAIN & DFTEST')
  print(df_train)
  print(df_test)

  m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)
  results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)
  for key in sorted(results):
    print("%s: %s" % (key, results[key]))
  
  #shutil.rmtree(model_dir)


def main(_):
  train_and_eval()

if __name__ == "__main__":
  tf.app.run()
